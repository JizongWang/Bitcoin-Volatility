# -*- coding: utf-8 -*-
"""Dissertation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gBUtqxofjIgHK8zKp0-BspBIPNyF0L8U

#Data

##install library
"""

# There are two data resourses
!pip install yfinance
!pip install nasdaq-data-link
!pip install arch

"""##price"""

#!pip install pandas
#!pip install yfinance
#!pip install seaborn
#!pip install matplotlib
#Load the required libraries
import pandas as pd
import yfinance as yf
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import nasdaqdatalink
from datetime import datetime
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels.tsa.api as smt
import scipy.stats as scs
import sklearn.metrics
from statsmodels.tsa.stattools import adfuller as adf

cryptocurrencies = ['BTC-USD']
data = yf.download(cryptocurrencies, start='2017-01-01',end='2022-08-01')  #interval = "1h"
data

# check for missing data
data.isnull().any()

"""###close price"""

# We are interested in the adjusted closing price. Therefore, we’ll select the adjusted-close price of the cryptocurrencies.
adj_close=data['Adj Close']
adj_close

# ploting the adjusted closing price
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(adj_close)
axs.set_title('BTC-price')
plt.show()

"""###return"""

# Returns i.e. percentage change in the adjusted close price and drop the first row with NA's.
# A return is a change in the price of an asset over time. Returns can be positive,
# representing a profit, or negative, indicating a loss. Return series of an asset are easier
# to handle than price series and give a complete summary of the investment opportunity.
returns = adj_close.pct_change().dropna(axis=0)
returns.head()

# ploting the returns
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(returns)
axs.set_title('BTC_returns')
axs.set_ylim([-0.5,0.5])
plt.show()

# standard deviation of the returns
#Volatility is a measure of change in the price of an asset over time. The higher the standard deviation, the more volatile an asset is.
returns.std()

# ploting the histogram
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.hist(returns, bins=50, range=(-0.2, 0.2))
axs.set_title('BTC_returns')
plt.show()

# Cumulative return series
# Cumulative return express the total change in the price of an asset over time.
cum_returns = ((1 + returns).cumprod() - 1) *100
cum_returns.head()
cum_returns.plot(figsize=(20,6))
plt.title('Cumulative Returns')

"""###log return"""

# log price
log_price = np.log(adj_close)
log_price.head()

# ploting the log_price
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(log_price)
axs.set_title('BTC_log_price')
plt.show()

# log return
log_return = log_price - log_price.shift(1)
log_return

# ploting the log_return
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(log_return)
axs.set_title('BTC_log_return')
axs.set_ylim([-0.5,0.5])
plt.show()

# Calculate squared log return
squared_log_return = np.power(log_return,2)
# ploting the squared log return
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(squared_log_return)
axs.set_title('BTC_squared_log_return')
plt.show()

"""###moving average"""

#Moving average (rolling average) is used to smooth out short-term fluctuations to identify long-term trends or cycles. For example, a 7-day moving average reflects short-term trends in the stock market, whereas a 200-day rolling average indicates major trends in the stock market. Here we calculate the arithmetic mean of a given set of prices over a specified period.
# compute a short-term 20-day moving average
MA20 = adj_close.rolling(20).mean()
# compute a Long-term 50-day moving average
MA50 = adj_close.rolling(100).mean()
# compute a Long-term 100-day moving average
MA100 = adj_close.rolling(100).mean()

# ploting the moving average
fig, axs = plt.subplots(figsize=(20,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(adj_close, label= 'closing')
axs.plot(MA20, label= 'MA20')
axs.plot(MA50, label= 'MA50')
axs.plot(MA100, label= 'MA100')
axs.set_title('BTC')
axs.legend()
plt.show()

"""##market capitalization"""

# import Bitcoin market capitalization data
import nasdaqdatalink
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

market_cap = nasdaqdatalink.get("BCHAIN/MKTCP",start_date="2017-01-01", end_date="2022-07-31",authtoken="HJdx-x76ZKQ97nrr9WRC")
market_cap.columns = ['market_cap']
market_cap.head()

# check for missing data
market_cap.isnull().any()

# ploting the market capitalization
fig,axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(market_cap)
axs.set_title('BTC_market_cap')
plt.show()

"""##mining revenue"""

# import Bitcoin mining revenue data
import nasdaqdatalink
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

mining_revenue = nasdaqdatalink.get("BCHAIN/MIREV",start_date="2017-01-01", end_date="2022-07-31",authtoken="HJdx-x76ZKQ97nrr9WRC")
mining_revenue.columns = ['mining_revenue']
mining_revenue.head()

# check for missing data
mining_revenue.isnull().any()

# ploting the mining revenue
fig,axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(mining_revenue)
axs.set_title('BTC_mining_revenue')
plt.show()

"""##volume"""

# import Bitcoin volume data
import nasdaqdatalink
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

volume = nasdaqdatalink.get("BCHAIN/TRVOU",start_date="2017-01-01", end_date="2022-07-31",authtoken="HJdx-x76ZKQ97nrr9WRC")
volume.columns = ['volume']
volume.head()

# check for missing data
volume.isnull().any()

# ploting the volume
fig,axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(volume)
axs.set_title('BTC_volume')
plt.show()

"""##hash rate"""

# import Bitcoin hash rate data
import nasdaqdatalink
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

hash_rate = nasdaqdatalink.get("BCHAIN/HRATE",start_date="2017-01-01", end_date="2022-07-31",authtoken="HJdx-x76ZKQ97nrr9WRC")
hash_rate.columns = ['hash_rate']
hash_rate.head()

# check for missing data
hash_rate.isnull().any()

# ploting the hash_rate
fig,axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(hash_rate)
axs.set_title('BTC_hash_rate')
plt.show()

"""##data merge and correlation calculation"""

# import Bitcoin daily close price data and combine the related variables with price
BTC_price = data[['Adj Close']]
BTC_price.columns = ['BTC_price']
BTC_data = BTC_price.merge(market_cap, on='Date', how='inner')
BTC_data = BTC_data.merge(mining_revenue, on='Date', how='inner')
BTC_data = BTC_data.merge(volume, on='Date', how='inner')
# BTC_data = BTC_data.merge(hash_rate, on='Date', how='inner')
BTC_data

# Correlation calculation
print(BTC_data.corr())

# plot
fig = pd.plotting.scatter_matrix(BTC_data,figsize=(8,8), c='blue', marker='o', diagonal='', alpha=0.8, range_padding=0.2)
plt.show()

# Plot the actual Bitcoin returns
fig, axs = plt.subplots(figsize=(16,8))
plt.plot(returns, color = 'grey', alpha = 0.4, label = 'Price_returns')
# plt.plot(adj_close, label = 'Price')
# plt.plot(market_cap, label = 'Market_cap')
plt.plot(volume, label = 'Volume')
plt.legend(loc = 'best')
plt.show()

#price and returns
fig,ax1 = plt.subplots(figsize=(16,8))
ax2 = ax1.twinx()           # 做镜像处理
ax1.plot(returns,'grey')
ax2.plot(adj_close,'b-')

ax1.set_xlabel('date')    #设置x轴标题
ax1.set_ylabel('Returns',color = 'grey')   #设置Y12轴标题
ax2.set_ylabel('Price',color = 'b')   #设置Y2轴标题

#market_cap and volume
fig,ax1 = plt.subplots(figsize=(16,8))
ax2 = ax1.twinx()           # 做镜像处理
ax1.plot(market_cap,'b-')
ax2.plot(volume,'g--')

ax1.set_xlabel('date')    #设置x轴标题
ax1.set_ylabel('Market_cap',color = 'b')   #设置Y12轴标题
ax2.set_ylabel('Volume',color = 'g')   #设置Y2轴标题

#adj_close，market_cap，mining_revenue，volume，hash_rate
stats_result = scs.describe(hash_rate,bias=False)
stats_result

result = hash_rate.describe()
result

"""## BTC/APPLE/GBP - USD"""

df_apple = yf.download('AAPL', start='2017-01-01',end='2022-07-31')
AAPL_price=df_apple['Adj Close']
AAPL_returns = AAPL_price.pct_change().dropna(axis=0)

# GU = ['GBPUSD']
df_GBP = yf.download('GBPUSD=X', start='2017-01-01',end='2022-07-31')
GBP_price=df_GBP['Adj Close']
GBP_returns = GBP_price.pct_change().dropna(axis=0)

# Plot the actual Bitcoin returns
fig, axs = plt.subplots(figsize=(16,8))
plt.plot(returns, color = 'grey', alpha = 0.4, label = 'BTC_returns')
plt.plot(AAPL_returns, label = 'AAPL_returns')
plt.plot(GBP_returns, label = 'GBP_returns')
plt.legend(loc = 'best')
plt.show()

#price and returns
fig,ax1 = plt.subplots(figsize=(16,8))
ax2 = ax1.twinx()           # 做镜像处理
ax1.plot(returns,'grey')
ax2.plot(AAPL_returns,'b-')

ax1.set_xlabel('date')    #设置x轴标题
ax1.set_ylabel('Returns',color = 'grey')   #设置Y12轴标题
ax2.set_ylabel('AAPL_returns',color = 'b')   #设置Y2轴标题

corr_matrix = np.corrcoef(df_apple['Close'], df_GBP['Close'])
corr_matrix

"""#GARCH Models"""

!pip install arch

"""##import daily data"""

df_adj = pd.read_csv("Bitstamp_BTCUSD_adj.csv")
df_adj["date"] = pd.to_datetime(df_adj["unix"],unit="s")
df_adj.set_index("date",inplace=True)
df_adj = df_adj.drop('unix', axis=1)
df_adj

# check for missing data
df_adj.isnull().any()

# check for duplicate values
df_adj.index.is_unique

# ploting the adjusted closing price
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(df_adj)
axs.set_title('BTC-price')
plt.show()

"""##Returns"""

# Returns i.e. percentage change in the adjusted close price and drop the first row with NA's.
# A return is a change in the price of an asset over time. Returns can be positive,
# representing a profit, or negative, indicating a loss. Return series of an asset are easier
# to handle than price series and give a complete summary of the investment opportunity.
adj_close = df_adj['close']
returns = adj_close.pct_change().dropna(axis=0)
plt.figure(figsize=(12,5));
plt.plot(returns);
plt.ylabel('Returns');
plt.title('Bitcoin Returns');

#One way to visualize the underlying volatility of the series is to plot the absolute returns ∣y∣:
plt.figure(figsize=(12,5))
plt.plot(np.abs(returns))
plt.ylabel('Absolute Returns')
plt.title('Bitcoin Absolute Returns')

def tsplot(y, lags=None, title='', figsize=(14, 8)):

    fig = plt.figure(figsize=figsize)
    layout = (3, 2)
    ts_ax   = plt.subplot2grid(layout, (0, 0))
    hist_ax = plt.subplot2grid(layout, (0, 1))
    pp_ax = plt.subplot2grid(layout, (1, 0))
    qq_ax = plt.subplot2grid(layout, (1, 1))
    acf_ax  = plt.subplot2grid(layout, (2, 0))
    pacf_ax = plt.subplot2grid(layout, (2, 1))

    y.plot(ax=ts_ax)
    ts_ax.set_title(title)
    y.plot(ax=hist_ax, kind='hist', bins=25)
    hist_ax.set_title('Histogram')
    sm.qqplot(y, line='s', ax=qq_ax)
    qq_ax.set_title('QQ Plot')
    scs.probplot(y, sparams=(y.mean(),y.std()), plot=pp_ax)
    pp_ax.set_title('PP Plot')
    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)
    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)
    [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]]
    sns.despine()
    plt.tight_layout()
    return ts_ax, acf_ax, pacf_ax

tsplot(returns, title='Bitcoin returns', lags=36);

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plot_acf(returns)
plot_pacf(returns)

"""##Log_return"""

#log_return
adj_close = df_adj['close']
log_price = np.log(adj_close)
log_return = log_price - log_price.shift(1)
log_return.columns = ['BTC_log_return']
log_return = log_return.drop(['2017-01-01'])
#log_return = 100*log_return
# ploting the log_return
fig, axs = plt.subplots(figsize=(12,5),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(log_return)
axs.set_title('BTC_log_return')
plt.show()

#Absolute log_return
plt.figure(figsize=(12,5))
plt.plot(np.abs(log_return))
plt.ylabel('Absolute log_return')
plt.title('Bitcoin Absolute log_return')

def tsplot(y, lags=None, title='', figsize=(14, 8)):

    fig = plt.figure(figsize=figsize)
    layout = (3, 2)
    ts_ax   = plt.subplot2grid(layout, (0, 0))
    hist_ax = plt.subplot2grid(layout, (0, 1))
    pp_ax = plt.subplot2grid(layout, (1, 0))
    qq_ax = plt.subplot2grid(layout, (1, 1))
    acf_ax  = plt.subplot2grid(layout, (2, 0))
    pacf_ax = plt.subplot2grid(layout, (2, 1))

    y.plot(ax=ts_ax)
    ts_ax.set_title(title)
    y.plot(ax=hist_ax, kind='hist', bins=25)
    hist_ax.set_title('Histogram')
    sm.qqplot(y, line='s', ax=qq_ax)
    qq_ax.set_title('QQ Plot')
    scs.probplot(y, sparams=(y.mean(),y.std()), plot=pp_ax)
    pp_ax.set_title('PP Plot')
    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)
    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)
    [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]]
    sns.despine()
    plt.tight_layout()
    return ts_ax, acf_ax, pacf_ax

tsplot(log_return, title='Bitcoin log_return', lags=36);

"""The autocorrelation is relatively weak, but after transforming it, such as taking the square, absolute value, etc., it shows a strong autocorrelation.

The conditional variance of the rate of return changes over time, that is, there is a characteristic of conditional heteroskedasticity.

The volatility of the yield sequence is persistent, that is, there is a phenomenon of volatility clustering. For example, around 2018 and from 2021 to 2022, there were greater volatility.

The QQ chart shows that the rate of return does not obey a normal distribution, with many extreme values and thick tails.

##Distinguishing samples
"""

sns.set(style='ticks', context='poster')
n_sample = log_return.shape[0]
print(log_return.shape)
print(log_return.head())

# Create a training sample and testing sample before analyzing the series (7:3)
n_train=int(0.7*n_sample)+1
n_forecast=n_sample-n_train
#ts_df
ts_train = log_return.iloc[:n_train]['BTC_log_return']
ts_test = log_return.iloc[n_train:]['BTC_log_return']
print(ts_train.shape)
print(ts_test.shape)
print("Training Series:", "\n", ts_train.tail(), "\n")
print("Testing Series:", "\n", ts_test.head())

"""##GARCH"""

#GARCH
from arch import arch_model
import datetime as dt
split_date = dt.datetime(2020,12,5)
am = arch_model(returns)
res = am.fit(last_obs=split_date)
print(res.summary())

df = pd.concat([res.resid, res.conditional_volatility], 1)
df.columns = ["Actual","GARCH"]
subplot = df.plot(figsize=(16,8))
plt.title('Fiting Performence')

res.resid.plot(figsize=(12,5))
plt.title('Bitcoin Returns Residuals',size=15)
plt.show()
res.conditional_volatility.plot(figsize=(12,5))
plt.title('Bitcoin Returns Conditional Variance',size=15)
plt.show()

df = pd.concat([d_df['realized_volatility_15min'][1:],res.conditional_volatility], 1)
df.columns = ["realized_volatility_15min","conditional_volatility"]
subplot = df.plot()
plt.title('Bitcoin Returns Conditional Volatility (GARCH)',size=12)

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plot_acf(res.conditional_volatility)
plot_pacf(res.conditional_volatility)

y_predict = res.forecast(horizon=1, start=split_date)
res_y = y_predict.variance

df = pd.concat([d_df['realized_volatility_15min'][split_date:], pow(res_y,0.5)], 1)
df.columns = ["Actual","Predict"]
subplot = df.plot(figsize=(16,8))
plt.title('Predict and Actual')

from sklearn.metrics import mean_absolute_error, mean_squared_error
def evaluate(observation, forecast):
    # Call sklearn function to calculate MAE
    mae = mean_absolute_error(observation, forecast)
    print(f'Mean Absolute Error (MAE): {round(mae,3)}')
    # Call sklearn function to calculate MSE
    mse = mean_squared_error(observation, forecast)
    print(f'Mean Squared Error (MSE): {round(mse,3)}')
    return mae, mse

# Backtest model with MAE, MSE
evaluate(d_df['realized_volatility_15min'][split_date:], pow(res_y,0.5))

"""##GJR-GARCH"""

#GJR-GARCH
from arch import arch_model
import datetime as dt
split_date = dt.datetime(2020,12,5)
am = arch_model(log_return,p=1, o=1, q=1)
gjr_res = am.fit(last_obs=split_date)
print(gjr_res.summary())

df = pd.concat([log_return,gjr_res.conditional_volatility], 1)
df.columns = ["log_return","conditional_volatility"]
subplot = df.plot()
plt.title('Bitcoin Returns Conditional Volatility (GJR-GARCH)',size=12)

"""##TGARCH"""

#TGARCH
from arch import arch_model
am = arch_model(log_return, p=1, o=1, q=1, power=1.0)
t_res = am.fit(update_freq=0)
print(t_res.summary())

df = pd.concat([log_return,t_res.conditional_volatility], 1)
df.columns = ["log_return","conditional_volatility"]
subplot = df.plot()
plt.title('Bitcoin Returns Conditional Volatility (T-GARCH)',size=12)

"""##EGARCH"""

#EGARCH
from arch import arch_model
am = arch_model(log_return, p=1, o=0, q=1, vol='EGARCH')
e_res = am.fit(update_freq=0)
print(e_res.summary())
from arch import arch_model
import datetime as dt
split_date = dt.datetime(2020,12,5)
am = arch_model(log_return, p=1, o=0, q=1, vol='EGARCH')
e_res = am.fit(last_obs=split_date)
print(e_res.summary())

df = pd.concat([log_return,e_res.conditional_volatility], 1)
df.columns = ["log_return","conditional_volatility"]
subplot = df.plot()
plt.title('Bitcoin Returns Conditional Volatility (E-GARCH)',size=12)

"""##Student’s T Errors

"""

#Student’s T Errors
from arch import arch_model
am = arch_model(log_return, p=1, o=1, q=1, power=1.0, dist="StudentsT")
t_t_res = am.fit(update_freq=0)
print(t_t_res.summary())

df = pd.concat([log_min_return,t_t_res.conditional_volatility], 1)
df.columns = ["log_min_return","conditional_volatility"]
subplot = df.plot()
plt.title('Bitcoin Returns Conditional Volatility (T-GARCH in Student’s Distribution)',size=12)

from arch import arch_model
am = arch_model(log_return, p=1, o=0, q=1, vol='EGARCH', dist="StudentsT")
res = am.fit(update_freq=0)
print(res.summary())

from arch import arch_model
am = arch_model(log_return, p=1, o=0, q=1, vol='EGARCH', dist="skewt")
e_s_res = am.fit(update_freq=0)
print(e_s_res.summary())

"""##GJR-GARCH(1,1) with Skew-t errors"""

#GJR-GARCH(1,1) with Skew-t errors
from arch import arch_model
am = arch_model(log_min_return, p=1, o=1, q=1, dist="skewt")
gjr_skewt_res = am.fit(update_freq=0)
print(gjr_skewt_res.summary())

df = pd.concat([log_min_return,gjr_skewt_res.conditional_volatility], 1)
df.columns = ["log_min_return","conditional_volatility"]
subplot = df.plot()
plt.title('Bitcoin Returns Conditional Volatility (GJR-GARCH in Skewed Student’s Distribution)',size=12)

"""##HARCH"""

#HARCH
from arch import arch_model
am = arch_model(log_min_return,vol='HARCH')
res = am.fit(update_freq=0)
print(res.summary())

df = pd.concat([log_return.sub(log_return.mean()).pow(2),res.conditional_volatility**2], 1)
df.columns = ["log_min_return","conditional_volatility"]
subplot = df.plot()
plt.title('Bitcoin Returns Conditional Volatility (GJR-GARCH in Skewed Student’s Distribution)',size=12)

"""##绘制重合图像"""

df = pd.concat([res.conditional_volatility, fixed_res.conditional_volatility], 1)
df.columns = ["Estimated", "Fixed"]
subplot = df.plot()
subplot.set_xlim(xlim)

# Plot the actual Bitcoin returns
fig, axs = plt.subplots(figsize=(16,8))
plt.plot(log_return, color = 'grey', alpha = 0.4, label = 'Price Log Returns')
plt.plot(res.conditional_volatility, label = 'EGARCH StuentsT')
plt.plot(e_s_res.conditional_volatility, label = 'EGARCH SkewT')
plt.plot(t_res.conditional_volatility, label = 'TGARCH Volatility')
plt.plot(e_res.conditional_volatility, label = 'EGARCH Volatility')
plt.plot(t_t_res.conditional_volatility, label = 'TGARCH StuentsT')
plt.legend(loc = 'down right')
plt.show()

"""#Evaluation"""

#Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE) and Root Mean Square Error (RMSE).
from sklearn.metrics import mean_absolute_error, mean_squared_error
from math import sqrt

def mae(observation, forecast):
    error = mean_absolute_error(observation, forecast)
    print('Mean Absolute Error (MAE): {:.3g}'.format(error))
    return error

def mape(observation, forecast):
    observation, forecast = np.array(observation), np.array(forecast)
    # Might encounter division by zero error when observation is zero
    error = np.mean(np.abs((observation - forecast) / observation)) * 100
    print('Mean Absolute Percentage Error (MAPE): {:.3g}'.format(error))
    return error

def rmse(observation, forecast):
    error = sqrt(mean_squared_error(observation, forecast))
    print('Root Mean Square Error (RMSE): {:.3g}'.format(error))
    return error

def evaluate(pd_dataframe, observation, forecast):
    """
    :params
    :pd_dataframe pandas dataframe
    :observation column name of expected values
    :forecast column name of forecasted values
    :return the results of MAE, MAPE and RMSE, respectively
    """
    first_valid_date = pd_dataframe[forecast].first_valid_index()
    mae_error = mae(pd_dataframe[observation].loc[first_valid_date:, ], pd_dataframe[forecast].loc[first_valid_date:, ])
    mape_error = mape(pd_dataframe[observation].loc[first_valid_date:, ], pd_dataframe[forecast].loc[first_valid_date:, ])
    rmse_error = rmse(pd_dataframe[observation].loc[first_valid_date:, ], pd_dataframe[forecast].loc[first_valid_date:, ])

    ax = pd_dataframe.loc[:, [observation, forecast]].plot()
    ax.xaxis.label.set_visible(False)

    return mae_error, mape_error, rmse_error

from sklearn.metrics import mean_absolute_error, mean_squared_error
def evaluate(observation, forecast):
    # Call sklearn function to calculate MAE
    mae = mean_absolute_error(observation, forecast)
    print(f'Mean Absolute Error (MAE): {round(mae,3)}')
    # Call sklearn function to calculate MSE
    mse = mean_squared_error(observation, forecast)
    print(f'Mean Squared Error (MSE): {round(mse,3)}')
    return mae, mse

# Backtest model with MAE, MSE
evaluate(log_min_return.sub(log_min_return.mean()).pow(2), res.conditional_volatility**2)

# Plot the actual Bitcoin volatility
plt.plot(log_return.sub(log_return.mean()).pow(2),
         color = 'grey', alpha = 0.4, label = 'Daily Volatility')

# Plot EGARCH  estimated volatility
plt.plot(res.conditional_volatility**2, color = 'red', label = 'EGARCH Volatility')

plt.legend(loc = 'upper right')
plt.show()

import numpy

corr_matrix = numpy.corrcoef(log_return.mean()).pow(2), gjr_skewt_res.conditional_volatility**2)
corr = corr_matrix[0,1]
R_sq = corr**2

print(R_sq)

"""#Minutes data

##import daily data
"""

df_adj = pd.read_csv("Bitstamp_BTCUSD_adj.csv")
df_adj["date"] = pd.to_datetime(df_adj["unix"],unit="s")
df_adj.set_index("date",inplace=True)
df_adj = df_adj.drop('unix', axis=1)
df_adj

# check for missing data
df_adj.isnull().any()

# check for duplicate values
df_adj.index.is_unique

# ploting the adjusted closing price
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(df_adj)
axs.set_title('BTC-price')
plt.show()

"""##import minute data"""

import pandas as pd

df_2017 = pd.read_csv("BTC-1min-2017.csv")
df_2017["date"] = pd.to_datetime(df_2017["unix"],unit="s")
df_2017.set_index("date",inplace=True)
df_2017 = df_2017.resample("1T").agg({"close":"last"})

df_2018 = pd.read_csv("BTC-1min-2018.csv")
df_2018["date"] = pd.to_datetime(df_2018["unix"],unit="s")
df_2018.set_index("date",inplace=True)
df_2018 = df_2018.resample("1T").agg({"close":"last"})

df_2019 = pd.read_csv("BTC-1min-2019.csv")
df_2019["date"] = pd.to_datetime(df_2019["unix"],unit="s")
df_2019.set_index("date",inplace=True)
df_2019 = df_2019.resample("1T").agg({"close":"last"})

df_2020 = pd.read_csv("BTC-1min-2020.csv")
df_2020["date"] = pd.to_datetime(df_2020["unix"],unit="s")
df_2020.set_index("date",inplace=True)
df_2020 = df_2020.resample("1T").agg({"close":"last"})

df_2021 = pd.read_csv("BTC-1min-2021.csv")
df_2021["date"] = pd.to_datetime(df_2021["unix"],unit="s")
df_2021.set_index("date",inplace=True)
df_2021 = df_2021.resample("1T").agg({"close":"last"})

df_2022 = pd.read_csv("BTC-1min-2022.csv")
df_2022["date"] = pd.to_datetime(df_2022["unix"],unit="s")
df_2022.set_index("date",inplace=True)
df_2022 = df_2022.resample("1T").agg({"close":"last"})

m_df = pd.concat([df_2017,df_2018,df_2019,df_2020,df_2021,df_2022])

# check for missing data
m_df.isnull().any()

# check for duplicate values
m_df.index.is_unique

m_df = m_df[~m_df.index.duplicated()]
m_df.index.is_unique

m_df.index.duplicated()

# ploting the adjusted closing price
fig, axs = plt.subplots(figsize=(16,8),gridspec_kw ={'hspace': 0.2, 'wspace': 0.1})
axs.plot(m_df)
axs.set_title('BTC-price')
plt.show()

"""##resample data"""

m_df['log_price'] = np.log(m_df.close)
#m_df['return'] = m_df.close.pct_change().dropna(axis=0)
m_df['log_return'] = m_df['log_price'] - m_df['log_price'].shift(1)

# Calculate squared log return
m_df['squared_log_return'] = np.power(m_df['log_return'], 2)

# Scale up 100x
#m_df['return_100x'] = np.multiply(m_df['return'], 100)
m_df['log_return_100x'] = np.multiply(m_df['log_return'], 100)
#m_df.loc[~m_df.index.duplicated(), :]

#adj_close = df_adj['close']
#returns = adj_close.pct_change().dropna(axis=0)
m_df['return'] = m_df['close'].pct_change()
m_df['squared_return'] = np.power(m_df['return'], 2)

# Realized bipower variation
r = np.append(np.nan, np.diff(np.log(m_df['close'])))
m_df['BV'] = np.multiply(np.absolute(r[:]), np.absolute(np.append(np.nan, r[:-1])))
m_df['BV'] = np.append(np.nan, m_df['BV'][0:-1]).reshape(-1,1)
m_df

# Resample to daily data
d_df = pd.DataFrame(m_df.loc[:, ['close']], index=m_df.index).resample('D', closed='left', label='left').mean().copy()
d_df['log_price'] = np.log(d_df.close)
d_df['return'] = d_df.close.pct_change().dropna()
d_df['log_return'] = d_df['log_price'] - d_df['log_price'].shift(1)
d_df['squared_log_return'] = np.power(d_df['log_return'], 2)

# Scale up 100x
d_df['return_100x'] = np.multiply(d_df['return'], 100)
d_df['log_return_100x'] = np.multiply(d_df['log_return'], 100)

d_df['realized_variance_1min'] = pd.Series(m_df.loc[:, 'squared_log_return'], index=m_df.index).resample('D', closed='left', label='left').sum().copy()
d_df['realized_volatility_1min'] = np.sqrt(d_df['realized_variance_1min'])

d_df['BV_1min'] = pd.Series(m_df.loc[:, 'BV'], index=m_df.index).resample('D', closed='left', label='left').sum().copy()

# Resample to 5-min data
five_min_df = pd.DataFrame(m_df.loc[:, ['close']], index=m_df.index).resample('5T', closed='left', label='left').mean().copy()
five_min_df['log_price'] = np.log(five_min_df.close)
five_min_df['log_return'] = five_min_df['log_price'] - five_min_df['log_price'].shift(1)
five_min_df['squared_log_return'] = np.power(five_min_df['log_return'], 2)
d_df['realized_variance_5min'] = pd.Series(five_min_df.loc[:, 'squared_log_return'], index=m_df.index).resample('D', closed='left', label='left').sum().copy()
d_df['realized_volatility_5min'] = np.sqrt(d_df['realized_variance_5min'])

# 15_min data
fivteen_min_df = pd.DataFrame(m_df.loc[:, ['close']], index=m_df.index).resample('15T', closed='left', label='left').mean().copy()
fivteen_min_df['log_price'] = np.log(fivteen_min_df.close)
fivteen_min_df['log_return'] = fivteen_min_df['log_price'] - fivteen_min_df['log_price'].shift(1)
fivteen_min_df['squared_log_return'] = np.power(fivteen_min_df['log_return'], 2)
d_df['realized_variance_15min'] = pd.Series(fivteen_min_df.loc[:, 'squared_log_return'], index=m_df.index).resample('D', closed='left', label='left').sum().copy()
d_df['realized_volatility_15min'] = np.sqrt(d_df['realized_variance_15min'])

# 1 hour data
one_hour_df = pd.DataFrame(m_df.loc[:, ['close']], index=m_df.index).resample('60T', closed='left', label='left').mean().copy()
one_hour_df['log_price'] = np.log(one_hour_df.close)
one_hour_df['log_return'] = one_hour_df['log_price'] - one_hour_df['log_price'].shift(1)
one_hour_df['squared_log_return'] = np.power(one_hour_df['log_return'], 2)
d_df['realized_variance_1h'] = pd.Series(one_hour_df.loc[:, 'squared_log_return'], index=m_df.index).resample('D', closed='left', label='left').sum().copy()
d_df['realized_volatility_1h'] = np.sqrt(d_df['realized_variance_1h'])
d_df

d_df[["realized_variance_1min","BV_1min"]].plot(title = 'RV-BV')
# plt.ylabel("RV")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.simplefilter('ignore')

# %matplotlib inline
# Increase chart resolution
# %config InlineBackend.figure_format = 'retina'
import seaborn as sns

sns.set_style('darkgrid')
sns.mpl.rcParams['figure.figsize'] = (20.0, 4.0)
sns.mpl.rcParams['savefig.dpi'] = 150
sns.mpl.rcParams['font.family'] = 'serif'
sns.mpl.rcParams['font.size'] = 12
sns.mpl.rcParams['lines.linewidth'] = 1.3

ax1 = d_df.loc[:, ['squared_log_return', 'realized_variance_1min', 'realized_variance_5min']].plot()
ax1.xaxis.label.set_visible(False)

def tsplot(y, lags=None, title='', figsize=(14, 8)):

    fig = plt.figure(figsize=figsize)
    layout = (3, 2)
    ts_ax   = plt.subplot2grid(layout, (0, 0))
    hist_ax = plt.subplot2grid(layout, (0, 1))
    pp_ax = plt.subplot2grid(layout, (1, 0))
    qq_ax = plt.subplot2grid(layout, (1, 1))
    acf_ax  = plt.subplot2grid(layout, (2, 0))
    pacf_ax = plt.subplot2grid(layout, (2, 1))

    y.plot(ax=ts_ax)
    ts_ax.set_title(title)
    y.plot(ax=hist_ax, kind='hist', bins=25)
    hist_ax.set_title('Histogram')
    sm.qqplot(y, line='s', ax=qq_ax)
    qq_ax.set_title('QQ Plot')
    scs.probplot(y, sparams=(y.mean(),y.std()), plot=pp_ax)
    pp_ax.set_title('PP Plot')
    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)
    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)
    [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]]
    sns.despine()
    plt.tight_layout()
    return ts_ax, acf_ax, pacf_ax

tsplot(d_df['log_return'][1:], title='Bitcoin log_return', lags=36);

log_RV_15 = d_df["realized_volatility_15min"]
tsplot(log_RV_15, title='Bitcoin RV', lags=36);

"""## Statistical description"""

adj_close = df_adj['close']
returns = pd.DataFrame(columns=['returns'])
returns['returns'] = adj_close.pct_change().dropna(axis=0)
#returns = pd.DataFrame(columns=['returns'])

adj_close = df_adj['close']
log_price = np.log(adj_close)
log_return = pd.DataFrame(columns=['log_return'])
log_return['log_return'] = log_price - log_price.shift(1)
#log_return.columns = ['log_return']
log_return = log_return.dropna()

fivteen_min_df = pd.DataFrame(m_df.loc[:, ['close']], index=m_df.index).resample('15T', closed='left', label='left').mean().copy()
fivteen_min_df['return'] = fivteen_min_df['close'].pct_change()
fivteen_min_df['squared_return'] = np.power(fivteen_min_df['return'], 2)
d_df_rv = pd.DataFrame(columns=['realized_variance_15min'])
d_df_rv['realized_variance_15min'] = pd.Series(fivteen_min_df.loc[:, 'squared_return'], index=m_df.index).resample('D', closed='left', label='left').sum().copy()
d_df_rv['realized_volatility_15min'] = np.sqrt(d_df_rv['realized_variance_15min'])
RV_15 = pd.DataFrame(columns=['RV_15'])
RV_15['RV_15'] = d_df_rv['realized_volatility_15min']

log_RV_15 = pd.DataFrame(columns=['log_RV_15'])
log_RV_15['log_RV_15'] = d_df["realized_volatility_15min"]

df_Stats = pd.concat([returns,log_return,RV_15,log_RV_15], 1)
df_Stats

print(df_Stats.describe())

# ADF Test ()
from statsmodels.tsa.stattools import adfuller
def adf_test(timeseries):
  print('Results of Dickey-Fuller Test:')
  dftest = adfuller(timeseries, autolag = 'AIC')
  dfoutput = pd.Series(dftest[0:4],index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
  for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
  print(dfoutput)

adf_test(df_Stats['log_RV_15'])

# Jarque–Bera test
# JB JBpv skewness kurtosis
from statsmodels.stats.stattools import jarque_bera
jarque_bera(df_Stats['log_RV_15'])

def tsplot(y, lags=None, title='', figsize=(14, 8)):

    fig = plt.figure(figsize=figsize)
    layout = (3, 2)
    ts_ax   = plt.subplot2grid(layout, (0, 0))
    hist_ax = plt.subplot2grid(layout, (0, 1))
    pp_ax = plt.subplot2grid(layout, (1, 0))
    qq_ax = plt.subplot2grid(layout, (1, 1))
    acf_ax  = plt.subplot2grid(layout, (2, 0))
    pacf_ax = plt.subplot2grid(layout, (2, 1))

    y.plot(ax=ts_ax)
    ts_ax.set_title(title)
    y.plot(ax=hist_ax, kind='hist', bins=25)
    hist_ax.set_title('Histogram')
    sm.qqplot(y, line='s', ax=qq_ax)
    qq_ax.set_title('QQ Plot')
    scs.probplot(y, sparams=(y.mean(),y.std()), plot=pp_ax)
    pp_ax.set_title('PP Plot')
    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)
    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)
    [ax.set_xlim(0) for ax in [acf_ax, pacf_ax]]
    sns.despine()
    plt.tight_layout()
    return ts_ax, acf_ax, pacf_ax

tsplot(log_return, title='Bitcoin log_return', lags=36);

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
#df = pd.concat([returns,d_df[["realized_volatility_15min"]]], 1)
#df.columns = ["returns","RV"]
#subplot = df.plot()
#plt.title('Bitcoin Returns Conditional Volatility (GARCH)',size=12)
plot_acf(d_df["realized_volatility_15min"])
plot_pacf(d_df["realized_volatility_15min"])

from statsmodels.tsa.stattools import pacf, acf
import plotly.graph_objects as go
def create_corr_plot(series, plot_pacf=False):
    corr_array = pacf(series.dropna(), alpha=0.05) if plot_pacf else acf(series.dropna(), alpha=0.05)
    lower_y = corr_array[1][:,0] - corr_array[0]
    upper_y = corr_array[1][:,1] - corr_array[0]

    fig = go.Figure()
    [fig.add_scatter(x=(x,x), y=(0,corr_array[0][x]), mode='lines',line_color='#3f3f3f')
     for x in range(len(corr_array[0]))]
    fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4',
                   marker_size=12)
    fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)')
    fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)',
            fill='tonexty', line_color='rgba(255,255,255,0)')
    fig.update_traces(showlegend=False)
    fig.update_xaxes(range=[-1,42])
    fig.update_yaxes(zerolinecolor='#000000')

    title='Partial Autocorrelation (PACF)' if plot_pacf else 'Autocorrelation (ACF)'
    fig.update_layout(title=title)
    fig.show()

create_corr_plot(d_df[["realized_volatility_15min"]])

"""# RV models

##HAR-RV
"""

rv = d_df[["realized_volatility_15min"]]
rv.columns=['RV_daily']
rv["RV_weekly"] = rv["RV_daily"].rolling(7).mean()
rv["RV_monthly"] = rv["RV_daily"].rolling(30).mean()
rv.dropna(inplace = True)
print(rv.head()); print(rv.describe())

# Check for stationarity with adf test
print("p-value for daily RV:", adf(rv["RV_daily"].values)[1])
print("p-value for weekly RV:",adf(rv["RV_weekly"].values)[1])
print("p-value for monthly RV:",adf(rv["RV_monthly"].values)[1])
#For all timeframes, the p-value is =< 0.01 thus, we reject the null hypothesis that the time series has a unit root. The time series is stationary.

# Plot the RV variables.
rv[["RV_daily","RV_weekly","RV_monthly"]].plot(title = 'RV_15min')
plt.ylabel("RV")
plt.show()

# Prepare data
rv["Target"] = rv["RV_daily"].shift(-1) #We want to predict the RV of the next day.
rv.dropna(inplace = True)
'''
#Scale the data
rv_scaled = (rv-rv.min())/(rv.max()-rv.min())

#Add constant c
rv_scaled = sm.add_constant(rv_scaled)

#Split train and test sets
split = int(0.70 * rv.shape[0])
X = rv_scaled.drop("Target", axis = 1)
y = rv_scaled[["Target"]]
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]
'''
#Split train and test sets
split = int(0.70 * rv.shape[0])
X = rv.drop("Target", axis = 1)
y = rv[["Target"]]
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

results = sm.OLS(y_train, X_train).fit()
#results = sm.OLS(y, X).fit()
results.summary()

# Perform out of sample prediction
y_hat = results.predict(X_test)
#y_hat = results.forecast(steps=15)

plt.figure(figsize = (14,5))

#Predicted RV
plt.subplot(1,2,1)
plt.plot(y_test.index, y_hat)
plt.title("Predicted RV")

#Actual RV
plt.subplot(1,2,2)
plt.plot(y_test.index, y_test, color = "orange")
plt.title("Actual RV")
plt.show()

plt.scatter(y_test, y_hat)
plt.title("Predicted vs observed RV")
plt.show()

# Metrics
def score(y_hat, y, metric):
    """Return metrics of y_hat vs. y

    Args:
        y_hat (np.array): Predicted values
        y (np.array): Actual values
        metric (str): Metric to use

    Returns:
        float: The metric
    """
    if metric == "MSE":
        return np.mean( (y_hat-y)**2)
    elif metric == "R_squared":
        ss_res = np.sum( (y - y_hat)**2 )
        ss_tot = np.sum( (y - np.average(y)) **2)
        return 1 - ss_res/ss_tot
    elif metric == "MAE":
        return np.mean( np.abs(y-y_hat))
#In-sample scores
print("In-sample scores")

y_hat_is = results.predict(X_train)
mse_is = score(y_hat_is, y_train.values.ravel(), "MSE")
r_sq_is = score(y_hat_is, y_train.values.ravel(), "R_squared")
mae_is = score(y_hat_is, y_train.values.ravel(), "MAE")
'''
y_hat_is = results.predict(X)
mse_is = score(y_hat_is, y.values.ravel(), "MSE")
r_sq_is = score(y_hat_is, y.values.ravel(), "R_squared")
mae_is = score(y_hat_is, y.values.ravel(), "MAE")
'''
print(f"MSE:{mse_is}, R^2:{r_sq_is}, MAE:{mae_is}")

print("----------------")

#Out-of-sample scores
print("Out-of-sample scores")

mse_oos = score(y_hat, y_test.values.ravel(), "MSE")
r_sq_oos = score(y_hat, y_test.values.ravel(), "R_squared")
mae_oos = score(y_hat, y_test.values.ravel(), "MAE")

print(f"MSE:{mse_oos}, R^2:{r_sq_oos}, MAE:{mae_oos}")

from sklearn.metrics import mean_absolute_error, mean_squared_error
def evaluate(observation, forecast):
    # Call sklearn function to calculate MAE
    mae = mean_absolute_error(observation, forecast)
    print(f'Mean Absolute Error (MAE): {round(mae,3)}')
    # Call sklearn function to calculate MSE
    mse = mean_squared_error(observation, forecast)
    print(f'Mean Squared Error (MSE): {round(mse,3)}')
    return mae, mse

# Backtest model with MAE, MSE
evaluate(y_test, y_hat)

df = pd.concat([y_test, y_hat], 1)
df.columns = ["RV (Actual)","HAR_RV (Predict)"]
subplot = df.plot(figsize=(16,8))
plt.title('Predict RV and Actual RV')

"""##HAR-GARCH-RV"""

# RV-HAR-GARCH
from arch import arch_model
from arch.univariate import HARX, Normal, GARCH
import datetime as dt
RV_HGARCH = d_df[["realized_volatility_15min"]]
split_date = dt.datetime(2020,12,5)
params = {'lags': [1, 7, 30], 'volatility': GARCH(), 'dist': Normal()}
am = HARX(RV_HGARCH, lags=params['lags'])
am.volatility = params['volatility']
am.distribution = params['dist']
HG_res = am.fit(last_obs=split_date)
print(HG_res.summary())

# RV-HAR-ARCH
from arch import arch_model
import datetime as dt
#from arch.univariate import HARCH
RV_HGARCH = d_df[["realized_volatility_15min"]]
split_date = dt.datetime(2020,12,4)
hm = arch_model(RV_HGARCH,vol='HARCH',lags=[1, 7, 30])
HG_res = hm.fit(last_obs=split_date)
print(HG_res.summary())

XV = HG_res.conditional_volatility.dropna()
XV

df = pd.concat([RV_HGARCH[:split_date], HG_res.conditional_volatility], 1)
df.columns = ["RV(Actual)","RV-HAR-GARCH(Predict)"]
subplot = df.plot(figsize=(16,8))
plt.title('Predict RV and Actual RV')

y_predict = HG_res.forecast(horizon=15, start=split_date)
HG_y = y_predict.variance

df = pd.concat([RV_HGARCH[split_date:], pow(HG_y,0.5)], 1)
df.columns = ["RV(Actual)","RV-HAR-GARCH(h1)","RV-HAR-GARCH(h2)","RV-HAR-GARCH(h3)","RV-HAR-GARCH(h4)","RV-HAR-GARCH(h5)","RV-HAR-GARCH(h6)","RV-HAR-GARCH(h7)","RV-HAR-GARCH(h8)","RV-HAR-GARCH(h9)","RV-HAR-GARCH(h10)","RV-HAR-GARCH(h11)","RV-HAR-GARCH(h12)","RV-HAR-GARCH(h13)","RV-HAR-GARCH(h14)","RV-HAR-GARCH(h15)"]
subplot = df.plot(figsize=(16,8))
plt.title('Predict RV and Actual RV')

from sklearn.metrics import mean_absolute_error, mean_squared_error
def evaluate(observation, forecast):
    # Call sklearn function to calculate MAE
    mae = mean_absolute_error(observation, forecast)
    print(f'Mean Absolute Error (MAE): {round(mae,3)}')
    # Call sklearn function to calculate MSE
    mse = mean_squared_error(observation, forecast)
    print(f'Mean Squared Error (MSE): {round(mse,3)}')
    return mae, mse

# Backtest model with MAE, MSE
evaluate(RV_HGARCH[split_date:], pow(HG_y["h.15"],0.5))

"""##GARCH-RV"""

# RV-GARCH
from arch import arch_model
import datetime as dt
RV_GARCH = d_df[["realized_volatility_15min"]]
split_date = dt.datetime(2020,12,5)

am = arch_model(RV_GARCH)
RG_res = am.fit(last_obs=split_date)
print(RG_res.summary())

RRGG = RG_res.conditional_volatility.dropna()
RRGG

#df = pd.concat([RV_GARCH[:split_date], RG_res.conditional_volatility], 1)
df = pd.concat([RG_res.resid, RG_res.conditional_volatility], 1)
df.columns = ["RV(Actual)","RV-GARCH"]
subplot = df.plot(figsize=(16,8))
plt.title('Fiting Performence')

y_predict = RG_res.forecast(horizon=15, start=split_date)
RG_y = y_predict.variance

df = pd.concat([RV_GARCH[split_date:], pow(RG_y,0.5)], 1)
df.columns = ["RV(Actual)","RV-GARCH(h1)","RV-GARCH(h2)","RV-GARCH(h3)","RV-GARCH(h4)","RV-GARCH(h5)","RV-GARCH(h6)","RV-GARCH(h7)","RV-GARCH(h8)","RV-GARCH(h9)","RV-GARCH(h10)","RV-GARCH(h11)","RV-GARCH(h12)","RV-GARCH(h13)","RV-GARCH(h14)","RV-GARCH(h15)"]
subplot = df.plot(figsize=(16,8))
plt.title('Predict RV and Actual RV')

from sklearn.metrics import mean_absolute_error, mean_squared_error
def evaluate(observation, forecast):
    # Call sklearn function to calculate MAE
    mae = mean_absolute_error(observation, forecast)
    print(f'Mean Absolute Error (MAE): {round(mae,3)}')
    # Call sklearn function to calculate MSE
    mse = mean_squared_error(observation, forecast)
    print(f'Mean Squared Error (MSE): {round(mse,3)}')
    return mae, mse

# Backtest model with MAE, MSE
evaluate(RV_GARCH[split_date:], pow(RG_y["h.15"],0.5))

"""## Out of samole Perfornmence"""

df = pd.concat([d_df['realized_volatility_15min'][split_date:],pow(res_y["h.1"],0.5),y_hat,pow(RG_y["h.01"],0.5),pow(HG_y["h.01"],0.5)], 1)
df.columns = ["RV","GARCH","HAR","RV-GARCH","RV-HAR-GARCH"]
subplot = df.plot(figsize=(16,8))
plt.title('volatility forecast')

"""##EGARCH-RV"""

#Normal distribution
from arch import arch_model
am = arch_model(d_df[["realized_volatility_15min"]], p=1, o=0, q=1, vol='EGARCH')
e_res = am.fit(update_freq=0)
print(e_res.summary())

#StudentsT distribution
from arch import arch_model
am = arch_model(d_df[["realized_volatility_15min"]], p=1, o=0, q=1, vol='EGARCH', dist="StudentsT")
e_T_res = am.fit(update_freq=0)
print(e_T_res.summary())

#SkewT distribution
from arch import arch_model
am = arch_model(d_df[["realized_volatility_15min"]], p=1, o=0, q=1, vol='EGARCH', dist="SkewT")
e_SkewT_res = am.fit(update_freq=0)
print(e_SkewT_res.summary())

from sklearn.metrics import mean_absolute_error, mean_squared_error
def evaluate(observation, forecast):
    # Call sklearn function to calculate MAE
    mae = mean_absolute_error(observation, forecast)
    print(f'Mean Absolute Error (MAE): {round(mae,3)}')
    # Call sklearn function to calculate MSE
    mse = mean_squared_error(observation, forecast)
    print(f'Mean Squared Error (MSE): {round(mse,3)}')
    return mae, mse

# Backtest model with MAE, MSE
evaluate(d_df['realized_volatility_5min'][1:].sub(d_df['realized_volatility_5min'][1:].mean()).pow(2), e_res.conditional_volatility**2)

EG = pd.concat([d_df['realized_volatility_5min'][1:], e_res.conditional_volatility], 1)
EG.columns = ["Actual","EGARCH_fit"]
subplot = EG.plot(figsize=(16,8))
plt.title('EGARCH',size=12)

"""##TGARCH-RV"""

#Normal distribution
from arch import arch_model
am = arch_model(d_df['log_return'][1:], p=1, o=1, q=1, power=1.0)
t_res = am.fit(update_freq=0)
print(t_res.summary())

#StudentsT distribution
from arch import arch_model
am = arch_model(d_df['log_return'][1:], p=1, o=1, q=1, power=1.0, dist="StudentsT")
t_T_res = am.fit(update_freq=0)
print(t_T_res.summary())

#SkewT distribution
from arch import arch_model
am = arch_model(d_df['log_return'][1:], p=1, o=1, q=1, power=1.0, dist="SkewT")
t_SkewT_res = am.fit(update_freq=0)
print(t_SkewT_res.summary())

from sklearn.metrics import mean_absolute_error, mean_squared_error
def evaluate(observation, forecast):
    # Call sklearn function to calculate MAE
    mae = mean_absolute_error(observation, forecast)
    print(f'Mean Absolute Error (MAE): {round(mae,3)}')
    # Call sklearn function to calculate MSE
    mse = mean_squared_error(observation, forecast)
    print(f'Mean Squared Error (MSE): {round(mse,3)}')
    return mae, mse

# Backtest model with MAE, MSE
evaluate(d_df['log_return'][1:].sub(d_df['log_return'][1:].mean()).pow(2), t_SkewT_res.conditional_volatility**2)

df = pd.concat([y, results.predict(X)], 1)
df.columns = ["log_min_return","conditional_volatility"]
subplot = df.plot(figsize=(16,8))
plt.title('Bitcoin Returns Conditional Volatility (GJR-GARCH in Skewed Student’s Distribution)',size=12)

HG = pd.concat([d_df['squared_log_return'], d_df['realized_variance_1min'][1:], d_df['realized_variance_15min'][1:], e_res.conditional_volatility**2, t_res.conditional_volatility**2], 1)
HG.columns = ["squared_log_return","RV_1min","RV_15min","EGARCH","TGARCH"]
HG = HG.drop(['2017-01-01'])
HG

fig = pd.plotting.scatter_matrix(HG,figsize=(8,8), c='blue', marker='o', diagonal='', alpha=0.8, range_padding=0.2)
plt.show()

#Granger's Causality Test
from statsmodels.tsa.stattools import grangercausalitytests
maxlag=12
test = 'ssr_chi2test'
def grangers_causation_matrix(data, variables, test='ssr_chi2test', verbose=False):
    """Check Granger Causality of all possible combinations of the Time series.
    The rows are the response variable, columns are predictors. The values in the table
    are the P-Values. P-Values lesser than the significance level (0.05), implies
    the Null Hypothesis that the coefficients of the corresponding past values is
    zero, that is, the X does not cause Y can be rejected.
    """
    df = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)
    for c in df.columns:
        for r in df.index:
            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)
            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]
            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')
            min_p_value = np.min(p_values)
            df.loc[r, c] = min_p_value
    df.columns = [var + '_x' for var in variables]
    df.index = [var + '_y' for var in variables]
    return df

grangers_causation_matrix(HG, variables = HG.columns)

import numpy as np
import matplotlib.pyplot as plt
from pandas import read_csv
import math
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
# convert an array of values into a dataset matrix
def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)
# fix random seed for reproducibility
tf.random.set_seed(7)
# load the dataset
dataframe = HG
dataset = dataframe.values
dataset = dataset.astype('float32')
# normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
dataset = scaler.fit_transform(dataset)
# split into train and test sets
train_size = int(len(dataset) * 0.67)
test_size = len(dataset) - train_size
train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
# reshape into X=t and Y=t+1
look_back = 1
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)
# reshape input to be [samples, time steps, features]
trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
# create and fit the LSTM network
model = Sequential()
model.add(LSTM(4, input_shape=(1, look_back)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)
# make predictions
trainPredict = model.predict(trainX)
testPredict = model.predict(testX)
# invert predictions
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])
# calculate root mean squared error
trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train Score: %.2f RMSE' % (trainScore))
testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test Score: %.2f RMSE' % (testScore))
# shift train predictions for plotting
trainPredictPlot = np.empty_like(dataset)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = np.empty_like(dataset)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
# plot baseline and predictions
plt.plot(scaler.inverse_transform(dataset))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

"""#Jump detection

##LeeMykland
"""

from math import ceil, sqrt
import numpy as np
import pandas as pd

def movmean(v, kb, kf):
    """
    Computes the mean with a window of length kb+kf+1 that includes the element
    in the current position, kb elements backward, and kf elements forward.
    Nonexisting elements at the edges get substituted with NaN.
    Args:
        v (list(float)): List of values.
        kb (int): Number of elements to include before current position
        kf (int): Number of elements to include after current position
    Returns:
        list(float): List of the same size as v containing the mean values
    """
    m = len(v) * [np.nan]
    for i in range(kb, len(v)-kf):
        m[i] = np.mean(v[i-kb:i+kf+1])
    return m


def LeeMykland(S, sampling, significance_level=0.01):
    """
    "Jumps in Equilibrium Prices and Market Microstructure Noise"
    - by Suzanne S. Lee and Per A. Mykland

    "https://galton.uchicago.edu/~mykland/paperlinks/LeeMykland-2535.pdf"

    Args:
        S (list(float)): An array containing prices, where each entry
                         corresponds to the price sampled every 'sampling' minutes.
        sampling (int): Minutes between entries in S
        significance_level (float): Defaults to 1% (0.001)

    Returns:
        A pandas dataframe containing a row covering the interval
        [t_i, t_i+sampling] containing the following values:
        J:   Binary value is jump with direction (sign)
        L:   L statistics
        T:   Test statistics
        sig: Volatility estimate
    """
    tm = 365*24*60 # Trading minutes
    k   = ceil(sqrt(tm/sampling))
    r = np.append(np.nan, np.diff(np.log(S)))
    bpv = np.multiply(np.absolute(r[:]), np.absolute(np.append(np.nan, r[:-1])))
    bpv = np.append(np.nan, bpv[0:-1]).reshape(-1,1) # Realized bipower variation
    sig = np.sqrt(movmean(bpv, k-3, 0)) # Volatility estimate
    L   = r/sig
    n   = np.size(S) # Length of S
    c   = (2/np.pi)**0.5
    Sn  = c*(2*np.log(n))**0.5
    Cn  = (2*np.log(n))**0.5/c - np.log(np.pi*np.log(n))/(2*c*(2*np.log(n))**0.5)
    beta_star   = -np.log(-np.log(1-significance_level)) # Jump threshold
    T   = (abs(L)-Cn)*Sn
    J   = (T > beta_star).astype(float)
    J   = J*np.sign(r) # Add direction
    # First k rows are NaN involved in bipower variation estimation are set to NaN.
    J[0:k] = np.nan
    # Build and retunr result dataframe
    return pd.DataFrame({'L': L,'sig': sig, 'T': T,'J':J})

close_price = m_df['close']
lee_jump = LeeMykland(close_price, 1, significance_level=0.1)
lee_jump

# lee_jump.J.plot()
# lee_jump.T.plot()
jj = lee_jump[['J']]
jj2 = jj.dropna()
# p = sum(i==0 for i in j)
# print(p)
jj2

s = lee_jump['J']!=0
s

fig = go.Figure()

fig.add_trace(go.Scatter(x=m_df.index, y=m_df['close'], mode='lines', name='Close Price'))

fig.add_trace(go.Scatter(x=lee_jump.index, y=s, mode='markers', name='Anomaly'))

fig.update_layout(showlegend=True)
fig.show()

import DailyVolDecomposition
import logging
import time
import multiprocessing
import os

def check_if_data_exists(symbol):
    data_path = 'Volatility Data/%s' % symbol

    if os.path.exists(data_path):
        return True

    return False

def main():

    symbols = [x[:-4] for x in os.listdir('Minute Prices/')]

    while True:
        print('Stock Tickers:')
        print(', '.join(symbols))
        print('\n')
        print('Type a stock symbol to calculate or "all" for all or "quit" to exit\n')
        inp = input('Input: ')
        inp = inp.upper()

        if inp == 'ALL':
            print('Starting...')
            symbols_to_calc = []
            #check if symbol already calculated
            for sym in symbols:
                if not check_if_data_exists(sym):
                    symbols_to_calc.append(sym)
            if len(symbols_to_calc) > 0:
                with multiprocessing.Pool(processes=None) as pool:
                    list_of_returns = pool.map(DailyVolDecomposition.process, symbols)
            print('All Done')
            print('\n')
            break

        elif inp in symbols:
            print('Starting...')
            DailyVolDecomposition.process(inp)
            print('All Done')
            print('\n')
        elif inp == 'QUIT':
            print('Bye Bye')
            break
        else:
            print('Input error, please retry')
            print('\n')

if __name__ == '__main__':

    log_file = 'Logs/run.log'
    if os.path.exists(log_file):
        os.remove(log_file)

    logging.basicConfig(filename=log_file, level=logging.DEBUG)
    logging.info('Starting at %s' % time.time())
    main()
    logging.info('Finished at %s' % time.time())

"""#Deep learning

## LSTM
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import tensorflow as tf
import pandas as pd
pd.options.mode.chained_assignment = None
import seaborn as sns
from matplotlib.pylab import rcParams
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go

# %matplotlib inline

sns.set(style='whitegrid', palette='muted')
rcParams['figure.figsize'] = 14, 8
np.random.seed(1)
tf.random.set_seed(1)

log_return = d_df[['log_return']]
df = log_return.drop(['2017-01-01'])
train_size = int(len(df) * 0.7)
test_size = len(df) - train_size
train, test = df.iloc[0:train_size], df.iloc[train_size:len(df)]
print(train.shape, test.shape)



"""Now, let's preprocess our datasets using Sk-learn Preprocessor which includes standardizing our target vector by removing the mean and scaling it to univariance"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler = scaler.fit(train[['log_return']])

train['log_return'] = scaler.transform(train[['log_return']])
test['log_return'] = scaler.transform(test[['log_return']])

"""Before we use this data to train our model, we need to split the data into subsequences because this is a time series modelling project. We can create specific sequences with specific time steps. In this case, we will use time steps as 30. We can create sequences worth 30 days of historical data and as required by LSTM, we need to reshape our input data into n times the shape and sample features. In our example, n-features is one and time steps to be 30. Creating a separate function to reshape data would be helpful to be able to re-use it over and again."""

def create_dataset(X, y, timestep=1):
    Xs, ys = [], []
    for i in range(len(X) - timestep):
        v = X.iloc[i:(i + timestep)].values
        Xs.append(v)
        ys.append(y.iloc[i + timestep])
    return np.array(Xs), np.array(ys)

"""Now let's create X_train, Y_train."""

time_steps = 30

X_train, y_train = create_dataset(train[['log_return']], train.log_return, time_steps)
X_test, y_test = create_dataset(test[['log_return']], test.log_return, time_steps)

print(X_train.shape)

"""The shape of the training data is (1396, 30, 1). Now, let's design and build our LSTM autoencoder. To detect anomalies, at first we train our data that does not contain anomalies and then take a new anomaly and try to reconstruct it using autoencoder. we will then use this reconstruction error for this new data point and if it is above a certain threshold that we set, then we would label the example data point as an anomaly. This is the reason why we are using LSTM network as it is good handling the temporal property. Now let's set the parameter first as the definition for LSTM autoencoder."""

timesteps = X_train.shape[1]
num_features = X_train.shape[2]

"""We will be using the Sequential model class from Keras API. The beauty of using the Sequential model plot is we can provide our definition as a list of layers to the Sequential class. The input to LSTM is a 2D array (timesteps, num_fatures) and the output of the layer is encoded feature vector of the input data. Let's also add some dropout regularization. You could change the hyperparameters to get better results. Since the subsequent layer is an LSTM, we need to duplicate this using a RepeatVector layer. This RepeatVector layer repeats the feature vector layer from the output of the LSTM timesteps times which is 30 for us. Now to create the decoder, we mirror the encoder in reverse fashion to create our decoder. Then we add Time distributed layer so that we can get the output of the right shape where the number of nodes is equal to number of features. The time distributed layer creates a vector equal to the number of layers output from the previous layer."""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed
from tensorflow.keras.optimizers import Adam

model = Sequential([
    LSTM(64, input_shape=(timesteps, num_features)),
    Dropout(0.2),
    RepeatVector(timesteps),
    LSTM(64, return_sequences=True),
    Dropout(0.2),
    TimeDistributed(Dense(num_features))
])
# 4 levels, 128 cells, lr=0.00055, dropout=0.2

# opt = Adam(learning_rate=0.0007)
model.compile(loss='mse', optimizer='Adam')
model.summary()

"""From the above model configuration, we can see that the Repeat Vector just duplicated the encoded representation 30 times. The time distributed layer decode the output layer and created 128 puls one as 129. Now let's train the model."""

es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40, mode='min')
history = model.fit(
    X_train, y_train,
    epochs=60,
    batch_size=64,
    validation_split=0.1,
    callbacks = [es],
    shuffle=False
)

"""Now let's plot the losses."""

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend();

#Evaluating our model
model.evaluate(X_test, y_test)

"""The loss calculated was 6.096. Now let's plot the training loss."""

X_train_pred = model.predict(X_train)
train_mae_loss = pd.DataFrame(np.mean(np.abs(X_train_pred - X_train), axis=1), columns=['Error'])
sns.distplot(train_mae_loss, bins=50, kde=True)

#test plot
X_test_pred = model.predict(X_test)
test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=1)
sns.distplot(test_mae_loss, bins=50, kde=True);

trainPredict = model.predict(X_train)
testPredict = model.predict(X_test)
nsamples, nx, ny = trainPredict.shape
trainPredict = trainPredict.reshape((nsamples,nx*ny))
nsamples, nx, ny = testPredict.shape
testPredict = testPredict.reshape((nsamples,nx*ny))

trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform(y_train.reshape(-1,1))
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform(y_test.reshape(-1,1))

plt.plot(trainY)
plt.plot(trainPredict[1:])
plt.show()
plt.plot(testY)
plt.plot(testPredict[1:])
plt.show()

THRESHOLD = 0.45

test_score_df = pd.DataFrame(test[time_steps:])
test_score_df['loss'] = test_mae_loss
test_score_df['threshold'] = THRESHOLD
test_score_df['anomaly'] = test_score_df.loss > test_score_df.threshold
test_score_df['realized_volatility_1h'] = test[time_steps:].realized_volatility_1h

fig = go.Figure()
fig.add_trace(go.Scatter(x=test[time_steps:].index, y=test_score_df.loss,
                    mode='lines',
                    name='Test Loss'))
fig.add_trace(go.Scatter(x=test[time_steps:].index, y=test_score_df.threshold,
                    mode='lines',
                    name='Threshold'))
fig.update_layout(showlegend=True)
fig.show()

anomalies = test_score_df[test_score_df.anomaly == True]
anomalies.head()

fig = go.Figure()

fig.add_trace(go.Scatter(x=test[time_steps:].index, y=scaler.inverse_transform(test[['realized_volatility_1h']][time_steps:]).reshape(-1), mode='lines', name='realized_volatility_1h'))

fig.add_trace(go.Scatter(x=anomalies.index, y=scaler.inverse_transform(anomalies[['realized_volatility_1h']]).reshape(-1), mode='markers', name='Anomaly'))

fig.update_layout(showlegend=True)
fig.show()

"""##其他"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
from torch import nn
from torch.autograd import Variable


#LSTM（Long Short-Term Memory）是长短期记忆网络
data_csv = d_df[['close']]
#pandas.read_csv可以读取CSV（逗号分割）文件、文本类型的文件text、log类型到DataFrame
#原有两列，时间和乘客数量，usecols=1：只取了乘客数量一列

plt.plot(data_csv)
plt.show()
#数据预处理
data_csv = data_csv.dropna() #去掉na数据
dataset = data_csv.values      #字典(Dictionary) values()：返回字典中的所有值。
dataset = dataset.astype('float32')   #astype(type):实现变量类型转换
max_value = np.max(dataset)
min_value = np.min(dataset)
scalar = max_value-min_value
dataset = list(map(lambda x: x/scalar, dataset)) #将数据标准化到0~1之间
#lambda:定义一个匿名函数，区别于def
#map(f(x),Itera):map()接收函数f和一个list,把函数f依次作用在list的每个元素上,得到一个新的object并返回



'''
接着我们进行数据集的创建，我们想通过前面几个月的流量来预测当月的流量，
比如我们希望通过前两个月的流量来预测当月的流量，我们可以将前两个月的流量
当做输入，当月的流量当做输出。同时我们需要将我们的数据集分为训练集和测试
集，通过测试集的效果来测试模型的性能，这里我们简单的将前面几年的数据作为
训练集，后面两年的数据作为测试集。
'''
def create_dataset(dataset,look_back=30):#look_back 以前的时间步数用作输入变量来预测下一个时间段
    dataX, dataY=[], []
    for i in range(len(dataset) - look_back):
        a = dataset[i:(i+look_back)]  #i和i+1赋值
        dataX.append(a)
        dataY.append(dataset[i+look_back])  #i+2赋值
    return np.array(dataX), np.array(dataY)  #np.array构建数组

data_X, data_Y = create_dataset(dataset)
#data_X: 2*142     data_Y: 1*142

#划分训练集和测试集，70%作为训练集
train_size = int(len(data_X) * 0.7)
test_size = len(data_X)-train_size

train_X = data_X[:train_size]
train_Y = data_Y[:train_size]

test_X = data_X[train_size:]
test_Y = data_Y[train_size:]

train_X = train_X.reshape(-1,1,30) #reshape中，-1使元素变为一行，然后输出为1列，每列2个子元素
train_Y = train_Y.reshape(-1,1,1) #输出为1列，每列1个子元素
test_X = test_X.reshape(-1,1,30)


train_x = torch.from_numpy(train_X) #torch.from_numpy(): numpy中的ndarray转化成pytorch中的tensor(张量)
train_y = torch.from_numpy(train_Y)
test_x = torch.from_numpy(test_X)


#定义模型 输入维度input_size是2，因为使用2个月的流量作为输入，隐藏层维度hidden_size可任意指定，这里为4
class lstm_reg(nn.Module):
    def __init__(self,input_size,hidden_size, output_size=1,num_layers=2):
        super(lstm_reg,self).__init__()
        #super() 函数是用于调用父类(超类)的一个方法，直接用类名调用父类
        self.rnn = nn.LSTM(input_size,hidden_size,num_layers) #LSTM 网络
        self.reg = nn.Linear(hidden_size,output_size) #Linear 函数继承于nn.Module
    def forward(self,x):   #定义model类的forward函数
        x, _ = self.rnn(x)
        s,b,h = x.shape   #矩阵从外到里的维数
                   #view()函数的功能和reshape类似，用来转换size大小
        x = x.view(s*b, h) #输出变为（s*b）*h的二维
        x = self.reg(x)
        x = x.view(s,b,-1) #卷积的输出从外到里的维数为s,b,一列
        return x

net = lstm_reg(30,4) #input_size=2，hidden_size=4

criterion = nn.MSELoss()  #损失函数均方差
optimizer = torch.optim.Adam(net.parameters(),lr=1e-2)
#构造一个优化器对象 Optimizer,用来保存当前的状态，并能够根据计算得到的梯度来更新参数
#Adam 算法:params (iterable)：可用于迭代优化的参数或者定义参数组的 dicts   lr:学习率


for e in range(10000):
    var_x = Variable(train_x) #转为Variable（变量）
    var_y = Variable(train_y)

    out = net(var_x)
    loss = criterion(out, var_y)

    optimizer.zero_grad() #把梯度置零，也就是把loss关于weight的导数变成0.
    loss.backward()  #计算得到loss后就要回传损失，这是在训练的时候才会有的操作，测试时候只有forward过程
    optimizer.step() #回传损失过程中会计算梯度，然后optimizer.step()根据这些梯度更新参数
    if (e+1)%100 == 0:
        print('Epoch: {}, Loss:{:.5f}'.format(e+1, loss.data))

torch.save(net.state_dict(), 'net_params.pkl') #保存训练文件net_params.pkl
#state_dict 是一个简单的python的字典对象,将每一层与它的对应参数建立映射关系

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
from torch import nn
from torch.autograd import Variable



# data_csv = d_df[['close']]

# # plt.plot(data_csv)
# # plt.show()
# #数据预处理

# data_csv = data_csv.dropna() #去掉na数据
# dataset = data_csv.values #字典(Dictionary) values()：返回字典中的所有值。
# dataset = dataset.astype('float32') # astype(type):实现变量类型转换
# max_value = np.max(dataset)
# min_value = np.min(dataset)
# scalar = max_value-min_value
# dataset = list(map(lambda x: x/scalar, dataset)) #将数据标准化到0~1之间

def create_dataset(dataset,look_back=2):
    dataX, dataY=[], []
    for i in range(len(dataset)-look_back):
        a=dataset[i:(i+look_back)]
        dataX.append(a)
        dataY.append(dataset[i+look_back])
    return np.array(dataX), np.array(dataY)

data_X, data_Y = create_dataset(dataset)


class lstm_reg(nn.Module):
    def __init__(self,input_size,hidden_size, output_size=1,num_layers=2):
        super(lstm_reg,self).__init__()

        self.rnn = nn.LSTM(input_size,hidden_size,num_layers)
        self.reg = nn.Linear(hidden_size,output_size)

    def forward(self,x):
        x, _ = self.rnn(x)
        s,b,h = x.shape
        x = x.view(s*b, h)
        x = self.reg(x)
        x = x.view(s,b,-1)
        return x


net = lstm_reg(30,4)

net.load_state_dict(torch.load('net_params.pkl'))

data_X = data_X.reshape(-1, 1, 2) #reshape中，-1使元素变为一行，然后输出为1列，每列2个子元素
data_X = torch.from_numpy(data_X) #torch.from_numpy(): numpy中的ndarray转化成pytorch中的tensor（张量）
var_data = Variable(data_X) #转为Variable（变量）
pred_test = net(var_data)  #产生预测结果
pred_test = pred_test.view(-1).data.numpy() #view(-1)输出为一行

plt.plot(pred_test, 'r', label='prediction')
plt.plot(dataset, 'b', label='real')
plt.legend(loc='best') #loc显示图像  'best'表示自适应方式
plt.show()

def load_data(df, seq_len):
    f = df
    data = f

    print('data len:',len(data))       #4172
    print('sequence len:',seq_len)     #50

    sequence_length = seq_len + 1
    result = []
    for index in range(len(data) - sequence_length):
        result.append(data[index: index + sequence_length])  #得到长度为seq_len+1的向量，最后一个作为label

    print('result len:',len(result))   #4121
    print('result shape:',np.array(result).shape)  #（4121,51）

    result = np.array(result)

    #划分train、test
    row = round(0.7 * result.shape[0])
    train = result[:row, :]
    np.random.shuffle(train)
    x_train = train[:, :-1]
    y_train = train[:, -1]
    x_test = result[row:, :-1]
    y_test = result[row:, -1]

    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

    print('X_train shape:',X_train.shape)  #(3709L, 50L, 1L)
    print('y_train shape:',y_train.shape)  #(3709L,)
    print('X_test shape:',X_test.shape)    #(412L, 50L, 1L)
    print('y_test shape:',y_test.shape)    #(412L,)

    return [x_train, y_train, x_test, y_test]

dd = load_data(d_df['close'], 30)

def build_model(layers):  #layers [1,50,100,1]
    model = Sequential()

    #Stack LSTM
    model.add(LSTM(input_dim=layers[0],output_dim=layers[1],return_sequences=True))
    model.add(Dropout(0.2))

    model.add(LSTM(layers[2],return_sequences=False))
    model.add(Dropout(0.2))

    model.add(Dense(output_dim=layers[3]))
    model.add(Activation("linear"))

    start = time.time()
    model.compile(loss="mse", optimizer="rmsprop")
    print("Compilation Time : ", time.time() - start)
    return model

md = build_model(dd)

def predict_point_by_point(model, data):
    predicted = model.predict(data)
    print('predicted shape:',np.array(predicted).shape)  #(412L,1L)
    predicted = np.reshape(predicted, (predicted.size,))
    return predicted

pre = predict_point_by_point(md, x_test)

plt.plot(testY)
plt.plot(pre)
plt.show()